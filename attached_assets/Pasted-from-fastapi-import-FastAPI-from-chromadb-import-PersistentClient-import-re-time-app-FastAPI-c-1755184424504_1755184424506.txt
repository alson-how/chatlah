from fastapi import FastAPI
from chromadb import PersistentClient
import re, time

app = FastAPI()
client = PersistentClient(path="./chroma")
col = client.get_collection("docs")

# ---- In-memory session store (replace with Redis/DB in prod)
SESS = {}  # {thread_id: {"summary": str, "turns": [(role, content), ...], "first_turn": bool}}

TONE_PROMPT = """You are replying as a Malaysian business owner from {company}. Use “I/we”.
Keep answers short (1–2 sentences), polite, friendly, professional. No exclamation marks,
no bold/bullets, no AI phrases. Only greet on first user greeting.
"""

def is_greeting(txt:str)->bool:
    return bool(re.match(r'^\s*(hi|hello|hey|morning|good (morning|afternoon|evening))\b', txt.strip(), re.I))

def postprocess(text:str, first_turn:bool)->str:
    # remove exclamation marks, overly generic assistant phrasing
    text = re.sub(r'\bassist you\b', 'help you', text, flags=re.I).replace('!', '').strip()
    if not first_turn:
        text = re.sub(r'^\s*(hi|hello|hey)[^a-z0-9]+', '', text, flags=re.I).strip()
    # cap 2 sentences
    parts = re.split(r'(?<=[.?!])\s+', text)
    return ' '.join(parts[:2]).strip()

# ---- LLM wrappers (replace with your client)
def llm_chat(messages, model="gpt-4o-mini", temperature=0.3, max_tokens=160):
    # pseudo: call OpenAI here
    # return openai.chat.completions.create(...)
    raise NotImplementedError

def summarise(previous_summary, user_msg, assistant_msg):
    prompt = f"""Update the conversation summary (<=500 tokens), capturing user intent, decisions, names,
preferences, and open items. First-person where relevant; no greetings.

previous_summary:
{previous_summary or ''}

latest_user:
{user_msg}

latest_assistant:
{assistant_msg}
"""
    out = llm_chat([
        {"role":"system","content":"You maintain a compact memory for continuity. Return only the updated summary."},
        {"role":"user","content": prompt}
    ], max_tokens=400)
    return out

def rewrite_query(summary, user_msg):
    prompt = f"""Rewrite the user's question into a standalone query for retrieval.
Use conversation_summary to resolve pronouns and include names/dates/scope.
Return only the rewritten query.

conversation_summary:
{summary or ''}

user_question:
{user_msg}
"""
    return llm_chat([
        {"role":"system","content":"You turn follow-up questions into standalone queries. Return only the query."},
        {"role":"user","content": prompt}
    ], max_tokens=120)

def retrieve(query, k=20):
    return col.query(query_texts=[query], n_results=k, include=["documents","metadatas","distances"])

def rerank(query, hits, topn=5):
    # Optional: apply a cross-encoder or heuristic MMR
    # For now, simple diversity by section + distance score
    docs = hits["documents"][0]; metas = hits["metadatas"][0]; dists = hits["distances"][0]
    triples = list(zip(docs, metas, dists))
    # naive: sort by distance then pick diverse sections
    triples.sort(key=lambda x: x[2])
    seen = set(); out=[]
    for t in triples:
        sec = t[1].get("section") if t[1] else None
        if sec in seen: continue
        seen.add(sec); out.append(t)
        if len(out)>=topn: break
    return out

def build_context(snippets):
    parts=[]
    for doc, meta, dist in snippets:
        title = meta.get("title","") if meta else ""
        parts.append(f"[{title}] {doc}")
    return "\n\n".join(parts[:3])

@app.post("/chat")
def chat(thread_id:str, user_message:str, name:str="Mei Yee", company:str="Jablanc Interior", portfolio_url:str=""):
    st = SESS.setdefault(thread_id, {"summary":"", "turns":[], "first_turn":True})
    first_turn = st["first_turn"]

    # Greeting short-circuit (no retrieval)
    if is_greeting(user_message):
        if first_turn:
            reply = f"Hi there, this is {name} here from {company}. How may I help you today?"
        else:
            reply = "Hi again—how can I help?"
        st["turns"].append(("user", user_message)); st["turns"].append(("assistant", reply))
        st["summary"] = summarise(st["summary"], user_message, reply)
        st["first_turn"] = False
        return {"answer": reply}

    # Query rewrite -> retrieval -> context
    rewritten = rewrite_query(st["summary"], user_message)
    hits = retrieve(rewritten)
    top = rerank(rewritten, hits, topn=5)
    context = build_context(top)

    # Build messages
    system_prompt = TONE_PROMPT.format(company=company)
    messages = [
        {"role":"system","content": system_prompt},
        {"role":"system","content": f"Conversation summary:\n{st['summary'][:2000]}"},
        {"role":"system","content": f"Relevant context (snippets):\n{context}"},
    ]
    # Add short history (last 2 turns)
    for role, content in st["turns"][-4:]:
        messages.append({"role":role, "content":content})
    messages.append({"role":"user","content": user_message})

    raw = llm_chat(messages, temperature=0.3, max_tokens=160)
    reply = postprocess(raw, first_turn=False)

    # Auto-append portfolio for “why choose” intent
    if re.search(r'\bwhy\b.*\bchoose\b', user_message, re.I) and portfolio_url and "Portfolio" not in reply:
        reply = f"{reply} Portfolio: {portfolio_url}"

    # Update memory
    st["turns"].append(("user", user_message)); st["turns"].append(("assistant", reply))
    st["summary"] = summarise(st["summary"], user_message, reply)
    st["first_turn"] = False

    # Optional: include source URLs back
    sources = []
    for _, meta, _ in top:
        if meta and "url" in meta: sources.append(meta["url"])
    return {"answer": reply, "sources": sources[:3]}